"""
ReAct Engine - Reasoning and Acting Loop
Core reasoning engine implementing the ReAct framework
"""
import asyncio
import json
import logging
import re
import time
from typing import Any, AsyncGenerator, Dict, List, Optional
from datetime import datetime

from ..config import settings
from ..schemas.agent import (
    AgentEvent, 
    AgentEventType, 
    StepResult, 
    ToolAction,
    ExecutionPlan,
    PlanStep,
    PlanStepStatus,
    CompletionResult
)
from .prompt_templates import (
    AGENT_SYSTEM_PROMPT,
    REACT_STEP_PROMPT,
    FINAL_ANSWER_TEMPLATE,
    ERROR_RECOVERY_PROMPT,
    PLAN_GENERATION_PROMPT,
    COMPLETION_CHECK_PROMPT,
    PLAN_GUIDED_REACT_PROMPT,
    format_history,
    format_observations,
    format_tool_list
)
from .skill_executor import skill_executor

logger = logging.getLogger(__name__)


class ReActEngine:
    """
    ReAct æ¨ç†å¼•æ“
    
    å®ç° Reasoning â†’ Action â†’ Observation å¾ªç¯
    """
    
    def __init__(
        self,
        llm_service=None,
        max_iterations: int = None,
        temperature: float = None
    ):
        """
        åˆå§‹åŒ– ReAct å¼•æ“
        
        Args:
            llm_service: LLM æœåŠ¡å®ä¾‹
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            temperature: LLM æ¸©åº¦å‚æ•°
        """
        self._llm_service = llm_service
        self.max_iterations = max_iterations or settings.AGENT_MAX_ITERATIONS
        self.temperature = temperature or settings.AGENT_DEFAULT_TEMPERATURE
        self.skill_executor = skill_executor
    
    @property
    def llm_service(self):
        """æ‡’åŠ è½½ LLM æœåŠ¡"""
        if self._llm_service is None:
            from ..services.llm_service import agent_llm_service
            self._llm_service = agent_llm_service
        return self._llm_service
    
    def _inject_context_params(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šå·¥å…·è‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡å‚æ•°
        
        ç›®å‰æ”¯æŒï¼š
        - memory_service: è‡ªåŠ¨æ³¨å…¥ session_idï¼ˆé™åˆ¶åœ¨å½“å‰ä¼šè¯å†…æœç´¢ï¼‰
        """
        if tool_name == "memory_service":
            # è‡ªåŠ¨æ³¨å…¥ session_id
            if "session_id" not in arguments and hasattr(self, '_current_context'):
                session_id = self._current_context.get("session_id")
                if session_id:
                    arguments = {**arguments, "session_id": session_id}
                    logger.debug(f"Auto-injected session_id for memory_service: {session_id}")
        
        return arguments
    
    async def run(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None,
        enabled_tools: Optional[List[str]] = None,
        disabled_tools: Optional[List[str]] = None
    ) -> AsyncGenerator[AgentEvent, None]:
        """
        æ‰§è¡Œ ReAct å¾ªç¯
        
        Args:
            task: ç”¨æˆ·ä»»åŠ¡/é—®é¢˜
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆçŸ¥è¯†ç©ºé—´ã€å†å²ç­‰ï¼‰
            enabled_tools: å¯ç”¨çš„å·¥å…·åˆ—è¡¨
            disabled_tools: ç¦ç”¨çš„å·¥å…·åˆ—è¡¨
            
        Yields:
            AgentEvent äº‹ä»¶æµ
        """
        start_time = time.time()
        history: List[Dict[str, Any]] = []
        observations: List[Dict[str, Any]] = []
        
        # ä¿å­˜å½“å‰ä¸Šä¸‹æ–‡ï¼Œä¾›å·¥å…·æ‰§è¡Œæ—¶ä½¿ç”¨
        self._current_context = context or {}
        
        # å‘é€å¼€å§‹äº‹ä»¶
        yield AgentEvent(
            type=AgentEventType.START,
            data={"task": task, "max_iterations": self.max_iterations}
        )
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        available_tools = self._get_available_tools(enabled_tools, disabled_tools)
        system_prompt = self._build_system_prompt(available_tools, context)
        
        # =====================================================================
        # Phase 1: Plan Generation (Plan-ReAct æ··åˆæ¨¡å‹)
        # =====================================================================
        execution_plan: Optional[ExecutionPlan] = None
        
        try:
            execution_plan = await self._generate_plan(task, available_tools)
            
            if execution_plan:
                # å‘é€è®¡åˆ’äº‹ä»¶
                yield AgentEvent(
                    type=AgentEventType.PLAN,
                    data={
                        "goal": execution_plan.goal,
                        "approach": execution_plan.approach,
                        "steps": [
                            {
                                "id": step.id,
                                "description": step.description,
                                "tool_hint": step.tool_hint,
                                "status": step.status.value
                            }
                            for step in execution_plan.steps
                        ],
                        "estimated_iterations": execution_plan.estimated_iterations
                    }
                )
                logger.info(f"Plan generated: {len(execution_plan.steps)} steps")
                
                # å¦‚æœè®¡åˆ’æ˜¾ç¤ºä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
                if not execution_plan.steps:
                    logger.info("No steps in plan, generating direct answer")
                    full_answer = ""
                    async for chunk in self._generate_final_answer(task, []):
                        full_answer += chunk
                        yield AgentEvent(
                            type=AgentEventType.FINAL_ANSWER,
                            data=chunk,
                            iteration=0
                        )
                    
                    yield AgentEvent(
                        type=AgentEventType.COMPLETE,
                        data={
                            "iterations": 0,
                            "total_time": time.time() - start_time,
                            "tools_used": [],
                            "plan_used": False
                        }
                    )
                    return
        except Exception as e:
            logger.warning(f"Plan generation failed, continuing with standard ReAct: {e}")
        
        # =====================================================================
        # Phase 2: ReAct Loop (å¸¦è®¡åˆ’å¼•å¯¼)
        # =====================================================================
        
        success_break = False
        for iteration in range(1, self.max_iterations + 1):
            logger.info(f"ReAct iteration {iteration}/{self.max_iterations}")
            
            # å‘é€è¿­ä»£å¼€å§‹äº‹ä»¶
            yield AgentEvent(
                type=AgentEventType.ITERATION,
                data={"iteration": iteration},
                iteration=iteration
            )
            
            try:
                # å‘é€æ—©æœŸæ€è€ƒçŠ¶æ€äº‹ä»¶ï¼ˆè®©ç”¨æˆ·ç«‹å³çœ‹åˆ°è¿›åº¦ï¼‰
                yield AgentEvent(
                    type=AgentEventType.THOUGHT,
                    data=f"æ­£åœ¨åˆ†æé—®é¢˜ï¼ˆç¬¬ {iteration} è½®ï¼‰...",
                    iteration=iteration
                )
                
                # 1. æ€è€ƒé˜¶æ®µï¼šè°ƒç”¨ LLM è¿›è¡Œæ¨ç†
                step_result = await self._think(
                    task, 
                    history, 
                    system_prompt,
                    execution_plan=execution_plan
                )
                
                # å‘é€æ€è€ƒç»“æœäº‹ä»¶
                yield AgentEvent(
                    type=AgentEventType.THOUGHT,
                    data=step_result.get("thought", ""),
                    iteration=iteration
                )
                
                # 2. æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                # 2. æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ (æ”¯æŒæ—§æ ¼å¼ key å’Œæ–°æ ¼å¼ Action)
                final_answer_signal = step_result.get("final_answer")
                
                # check for action tool = final_answer
                actions_check = []
                if step_result.get("action"): actions_check = [step_result["action"]]
                elif step_result.get("actions"): actions_check = step_result["actions"]
                
                for act in actions_check:
                    if isinstance(act, dict) and act.get("tool") == "final_answer":
                        final_answer_signal = act.get("thought", "Ready to answer")
                        break

                if final_answer_signal:
                    # =========================================================
                    # å®Œæˆåº¦æ£€æŸ¥ (Completion Check)
                    # =========================================================
                    
                    # å¦‚æœæ˜¯ Action signal (Ready to answer)ï¼Œè¯´æ˜ Agent æ˜ç¡®è¡¨ç¤ºå·²å®Œæˆå¹¶å‡†å¤‡å›ç­”
                    # æ­¤æ—¶ä¸éœ€è¦è¿›è¡Œæ–‡æœ¬å®Œæ•´æ€§æ£€æŸ¥ï¼Œç›´æ¥è®¤ä¸ºå®Œæˆ
                    if str(final_answer_signal) == "Ready to answer":
                        completion_result = CompletionResult(
                            is_complete=True,
                            confidence=1.0,
                            reasoning="Agent explicit final_answer signal",
                            missing_items=[],
                            suggested_next_steps=[]
                        )
                    else:
                        # å¦åˆ™å¯¹ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬è¿›è¡Œæ£€æŸ¥
                        completion_result = await self._check_completion(
                            task=task,
                            plan=execution_plan,
                            observations=observations,
                            current_answer=str(final_answer_signal)
                        )
                    
                    # å‘é€å®Œæˆåº¦æ£€æŸ¥äº‹ä»¶
                    yield AgentEvent(
                        type=AgentEventType.COMPLETION_CHECK,
                        data={
                            "is_complete": completion_result.is_complete,
                            "confidence": completion_result.confidence,
                            "reasoning": completion_result.reasoning,
                            "missing_items": completion_result.missing_items,
                            "suggested_next_steps": completion_result.suggested_next_steps
                        },
                        iteration=iteration
                    )
                    
                    # å¦‚æœæœªå®Œæˆä¸”è¿˜æœ‰è¿­ä»£æ¬¡æ•°ï¼Œç»§ç»­æ‰§è¡Œ
                    if not completion_result.is_complete and iteration < self.max_iterations:
                        logger.info(f"Completion check: Not complete (confidence: {completion_result.confidence}), continuing...")
                        # å°†å»ºè®®çš„ä¸‹ä¸€æ­¥æ·»åŠ åˆ°æ€è€ƒå†å²ï¼Œå¼•å¯¼ LLM
                        if completion_result.suggested_next_steps:
                            history.append({
                                "thought": f"å®Œæˆåº¦æ£€æŸ¥ï¼šæœªå®Œæˆï¼Œç¼ºå°‘ {', '.join(completion_result.missing_items) if completion_result.missing_items else 'æŸäº›ä¿¡æ¯'}",
                                "action": None,
                                "observation": {"result": f"å»ºè®®: {', '.join(completion_result.suggested_next_steps)}"}
                            })
                        continue  # ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£
                    
                    # å®Œæˆæˆ–è¾¾åˆ°ä¸Šé™ -> é€€å‡ºå¾ªç¯ï¼Œè¿›å…¥ç»Ÿä¸€çš„ Final Answer ç”Ÿæˆæµç¨‹
                    success_break = True
                    break
                
                # 3. è¡ŒåŠ¨é˜¶æ®µï¼šæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¹¶è¡Œï¼‰
                actions_to_execute = []
                
                # æ”¯æŒå•ä¸ª action
                if step_result.get("action"):
                    actions_to_execute = [step_result["action"]]
                # æ”¯æŒå¹¶è¡Œ actions æ•°ç»„
                elif step_result.get("actions"):
                    actions_to_execute = step_result["actions"]
                
                # è¿‡æ»¤æ— æ•ˆçš„åŠ¨ä½œ
                actions_to_execute = [a for a in actions_to_execute if isinstance(a, dict)]
                
                # CRITICAL Fix: å¦‚æœæ²¡æœ‰åŠ¨ä½œï¼Œå¿…é¡»æ›´æ–°å†å²ï¼Œå¦åˆ™ LLM ä¼šé™·å…¥æ­»å¾ªç¯
                if not actions_to_execute:
                    logger.warning("No actions generated in this step.")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿ç»­å¤šæ¬¡æ²¡æœ‰åŠ¨ä½œ (æ­»å¾ªç¯æ£€æµ‹)
                    consecutive_no_action = 0
                    repeated_thought = False
                    
                    # æ£€æŸ¥æ€è€ƒæ˜¯å¦é‡å¤
                    current_thought = step_result.get("thought", "").strip()
                    if history and history[-1].get("thought", "").strip() == current_thought:
                        repeated_thought = True
                        logger.warning(f"Repeated thought detected: {current_thought[:50]}...")
                    
                    for h in reversed(history):
                        if h.get("action") is None and not h.get("actions"):
                            consecutive_no_action += 1
                        else:
                            break
                    
                    # é™ä½é˜ˆå€¼ï¼šå¦‚æœæ€è€ƒé‡å¤ä¸”æ— åŠ¨ä½œï¼Œç«‹å³å¹²é¢„
                    if consecutive_no_action >= 2 or (repeated_thought and consecutive_no_action >= 1):
                        logger.warning(f"Detected potential infinite loop (no actions for {consecutive_no_action} steps, repeated={repeated_thought}). Forcing final answer prompt.")
                        # å¼ºåˆ¶æ³¨å…¥ä¸€ä¸ª Observationï¼Œå¼•å¯¼ LLM è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
                        history.append({
                            "thought": step_result.get("thought", "No thought"),
                            "action": None,
                            "observation": {"result": "SYSTEM WARNING: You are in a loop of thinking without acting. Stop thinking and provide the Final Answer immediately using the 'final_answer' JSON format. DO NOT generate more thoughts."}
                        })
                    else:
                        instruction = "No action taken."
                        if repeated_thought:
                            instruction += " You just had this exact same thought. Please change your strategy or provide a Final Answer."
                        else:
                            instruction += " If you have enough information or no tools are needed, you MUST output a JSON with 'final_answer'."
                            
                        history.append({
                            "thought": step_result.get("thought", "No thought"),
                            "action": None,
                            "observation": {"result": instruction}
                        })
                    continue

                if actions_to_execute:
                    # Check for duplicate actions to prevent infinite loops
                    unique_actions = []
                    duplicate_observations = []
                    
                    for action in actions_to_execute:
                        if self._is_duplicate_action(action, history):
                            logger.warning(f"Duplicate action detected: {action}")
                            # Create a fake observation for the duplicate action
                            dup_obs = {
                                "success": False,
                                "error": "SYSTEM WARNING: You have already executed this exact action in this session. Please modify your query or strategy significantly.",
                                "result": None
                            }
                            # Dispatch observation event immediately
                            yield AgentEvent(
                                type=AgentEventType.OBSERVATION,
                                data=dup_obs,
                                iteration=iteration,
                                tool_name=action.get("tool"),
                                execution_time=0
                            )
                            duplicate_observations.append({
                                "tool": action.get("tool"),
                                "action": action,
                                "result": dup_obs
                            })
                        else:
                            unique_actions.append(action)
                    
                    # If all actions were duplicates, update history and continue
                    if not unique_actions:
                         history.append({
                            "thought": step_result.get("thought"),
                            "actions": actions_to_execute, 
                            "observations": duplicate_observations
                        })
                         continue
                        
                    actions_to_execute = unique_actions
                    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·
                    if len(actions_to_execute) > 1:
                        logger.info(f"Executing {len(actions_to_execute)} actions in parallel")
                        
                        # å‘é€å¹¶è¡Œè¡ŒåŠ¨å¼€å§‹äº‹ä»¶
                        for action in actions_to_execute:
                            yield AgentEvent(
                                type=AgentEventType.ACTION,
                                data={
                                    "tool": action.get("tool"),
                                    "method": action.get("method", "execute"),
                                    "arguments": action.get("arguments", {}),
                                    "parallel": True
                                },
                                iteration=iteration,
                                tool_name=action.get("tool")
                            )
                        
                        # å¹¶è¡Œæ‰§è¡Œ
                        async def execute_action(act):
                            try:
                                # è‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡å‚æ•°ï¼ˆå¦‚ memory_service çš„ session_idï¼‰
                                tool_name = act.get("tool")
                                arguments = self._inject_context_params(tool_name, act.get("arguments", {}))
                                
                                return await asyncio.wait_for(
                                    self.skill_executor.execute(
                                        skill_name=tool_name,
                                        method=act.get("method", "execute"),
                                        arguments=arguments
                                    ),
                                    timeout=settings.AGENT_TOOL_TIMEOUT
                                )
                            except asyncio.TimeoutError:
                                return {
                                    "success": False,
                                    "error": f"å·¥å…· {act.get('tool')} æ‰§è¡Œè¶…æ—¶",
                                    "result": None,
                                    "execution_time": settings.AGENT_TOOL_TIMEOUT
                                }
                        
                        execution_results = await asyncio.gather(
                            *[execute_action(act) for act in actions_to_execute],
                            return_exceptions=True
                        )
                        
                        # å‘é€æ‰€æœ‰è§‚å¯Ÿäº‹ä»¶
                        combined_observations = []
                        for i, (action, result) in enumerate(zip(actions_to_execute, execution_results)):
                            if isinstance(result, Exception):
                                result = {"success": False, "error": str(result), "result": None}
                            
                            yield AgentEvent(
                                type=AgentEventType.OBSERVATION,
                                data=result,
                                iteration=iteration,
                                tool_name=action.get("tool"),
                                execution_time=result.get("execution_time")
                            )
                            
                            combined_observations.append({
                                "tool": action.get("tool"),
                                "action": action,
                                "result": result
                            })
                            
                            if result.get("success"):
                                observations.append({
                                    "tool": action.get("tool"),
                                    "result": result.get("result")
                                })
                        
                        # æ›´æ–°å†å²ï¼ˆå¹¶è¡Œç»“æœåˆå¹¶ï¼‰
                        history.append({
                            "thought": step_result.get("thought"),
                            "actions": actions_to_execute,
                            "observations": combined_observations
                        })
                    
                    else:
                        # å•ä¸ªå·¥å…·æ‰§è¡Œï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                        action = actions_to_execute[0]
                        tool_name = action.get("tool")
                        method = action.get("method", "execute")
                        arguments = action.get("arguments", {})
                        
                        # è‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡å‚æ•°ï¼ˆå¦‚ memory_service çš„ session_idï¼‰
                        arguments = self._inject_context_params(tool_name, arguments)
                        
                        # å‘é€è¡ŒåŠ¨äº‹ä»¶
                        yield AgentEvent(
                            type=AgentEventType.ACTION,
                            data={
                                "tool": tool_name,
                                "method": method,
                                "arguments": arguments
                            },
                            iteration=iteration,
                            tool_name=tool_name
                        )
                        
                        # æ‰§è¡Œå·¥å…·ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
                        try:
                            execution_result = await asyncio.wait_for(
                                self.skill_executor.execute(
                                    skill_name=tool_name,
                                    method=method,
                                    arguments=arguments
                                ),
                                timeout=settings.AGENT_TOOL_TIMEOUT
                            )
                        except asyncio.TimeoutError:
                            logger.warning(f"Tool {tool_name} timeout after {settings.AGENT_TOOL_TIMEOUT}s")
                            execution_result = {
                                "success": False,
                                "error": f"å·¥å…· {tool_name} æ‰§è¡Œè¶…æ—¶ï¼ˆ{settings.AGENT_TOOL_TIMEOUT}ç§’ï¼‰",
                                "result": None,
                                "execution_time": settings.AGENT_TOOL_TIMEOUT
                            }
                        
                        # å‘é€è§‚å¯Ÿäº‹ä»¶
                        yield AgentEvent(
                            type=AgentEventType.OBSERVATION,
                            data=execution_result,
                            iteration=iteration,
                            tool_name=tool_name,
                            execution_time=execution_result.get("execution_time")
                        )
                        
                        # æ›´æ–°å†å²
                        history.append({
                            "thought": step_result.get("thought"),
                            "action": action,
                            "observation": execution_result
                        })
                        
                        # è®°å½•æˆåŠŸçš„è§‚å¯Ÿç»“æœ
                        if execution_result.get("success"):
                            observations.append({
                                "tool": tool_name,
                                "result": execution_result.get("result")
                            })
                        
                        # å¦‚æœå·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œå°è¯•é”™è¯¯æ¢å¤
                        if not execution_result.get("success"):
                            recovery_action = await self._handle_error(
                                task, action, execution_result.get("error", "Unknown error")
                            )
                            if recovery_action:
                                history[-1]["recovery"] = recovery_action
                

            except Exception as e:
                logger.error(f"ReAct iteration {iteration} failed: {e}")
                yield AgentEvent(
                    type=AgentEventType.ERROR,
                    data={"error": str(e), "iteration": iteration}
                )
        
        # å¾ªç¯ç»“æŸï¼ˆå¯èƒ½æ˜¯ break ä¹Ÿå¯èƒ½æ˜¯è¾¾åˆ°ä¸Šé™ï¼‰
        if not success_break:
            logger.warning(f"Reached max iterations ({self.max_iterations})")
        
        # 1. Start streaming final answer
        try:
            full_content = ""
            async for chunk in self._generate_final_answer(task, observations):
                full_content += chunk
                yield AgentEvent(
                    type=AgentEventType.FINAL_ANSWER,
                    data=chunk,  # Yield chunk
                    iteration=self.max_iterations
                )
        except Exception as e:
            logger.error(f"Error generating final answer: {e}")
            yield AgentEvent(
                type=AgentEventType.FINAL_ANSWER,
                data="ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡ºé”™",
                iteration=self.max_iterations
            )

        yield AgentEvent(
            type=AgentEventType.COMPLETE,
            data={
                "iterations": self.max_iterations,
                "total_time": time.time() - start_time,
                "tools_used": [obs["tool"] for obs in observations],
                "max_iterations_reached": True
            }
        )
    
    def _get_available_tools(
        self,
        enabled_tools: Optional[List[str]],
        disabled_tools: Optional[List[str]]
    ) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        all_tools = self.skill_executor.get_available_tools()
        
        if enabled_tools is not None:
            all_tools = [t for t in all_tools if t["name"] in enabled_tools]
        
        if disabled_tools:
            all_tools = [t for t in all_tools if t["name"] not in disabled_tools]
        
        return all_tools
    
    def _build_system_prompt(
        self,
        available_tools: List[Dict[str, Any]],
        context: Optional[Dict[str, Any]]
    ) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        tools_text = format_tool_list(available_tools)
        context_text = self._format_context(context) if context else "æ— é¢å¤–ä¸Šä¸‹æ–‡"
        
        return AGENT_SYSTEM_PROMPT.format(
            available_tools=tools_text,
            context=context_text,
            current_date=datetime.now().strftime("%Y-%m-%d")
        )
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        parts = []
        
        # ä¼šè¯ä¿¡æ¯
        if "session_id" in context:
            turn_count = context.get('turn_count', 0)
            message_count = context.get('message_count', 0)
            session_info = f"**å½“å‰ä¼šè¯**: session_id=`{context['session_id']}`, å·²æœ‰ {turn_count} è½®å¯¹è¯ï¼ˆå…± {message_count} æ¡æ¶ˆæ¯ï¼‰"
            if turn_count > 4:
                session_info += "\n> ğŸ’¡ å¦‚éœ€å›é¡¾å†å²å¯¹è¯ï¼Œè¯·ä½¿ç”¨ `memory_service.search()` æˆ– `memory_service.get_recent()`"
            parts.append(session_info)
        
        # ç®€çŸ­å¯¹è¯çš„æœ€è¿‘ä¸Šä¸‹æ–‡ï¼ˆè‡ªåŠ¨æ³¨å…¥ï¼‰
        if "recent_context" in context:
            parts.append(f"**æœ€è¿‘å¯¹è¯ï¼š**\n{context['recent_context']}")
        
        # å›¾ç‰‡åˆ†æç»“æœ
        if "image_analysis" in context:
            parts.append(f"**å›¾ç‰‡åˆ†æï¼š**\n{context['image_analysis']}")
        
        return "\n\n".join(parts) if parts else "æ–°ä¼šè¯ï¼Œæ— å†å²ä¸Šä¸‹æ–‡"
    
    async def _think(
        self,
        task: str,
        history: List[Dict[str, Any]],
        system_prompt: str,
        execution_plan: Optional[ExecutionPlan] = None
    ) -> Dict[str, Any]:
        """
        æ€è€ƒé˜¶æ®µï¼šè°ƒç”¨ LLM è¿›è¡Œæ¨ç†
        
        Returns:
            åŒ…å« thought å’Œ action/final_answer çš„å­—å…¸
        """
        # æ„å»ºæ¶ˆæ¯
        user_prompt = REACT_STEP_PROMPT.format(
            task=task,
            history=format_history(history)
        )
        
        # å¦‚æœæœ‰è®¡åˆ’ï¼Œä½¿ç”¨è®¡åˆ’å¼•å¯¼çš„æç¤ºè¯
        if execution_plan:
            current_step = execution_plan.get_current_step()
            step_desc = current_step.description if current_step else "æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ"
            plan_summary = self._format_plan_summary(execution_plan)
            
            user_prompt = PLAN_GUIDED_REACT_PROMPT.format(
                task=task,
                plan_summary=plan_summary,
                current_step=step_desc,
                history=format_history(history)
            )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # è°ƒç”¨ LLMï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
        # æ€è€ƒé˜¶æ®µä½¿ç”¨è¾ƒå°‘çš„ max_tokens åŠ å¿«å“åº”ï¼Œæœ€ç»ˆå›ç­”é˜¶æ®µå†ä½¿ç”¨å®Œæ•´ tokens
        try:
            response = await asyncio.wait_for(
                self.llm_service.chat_completion(
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=2048  # æ€è€ƒé˜¶æ®µåªéœ€è¦ç®€çŸ­è¾“å‡ºï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                ),
                timeout=settings.AGENT_LLM_CALL_TIMEOUT
            )
        except asyncio.TimeoutError:
            logger.warning(f"LLM call timeout after {settings.AGENT_LLM_CALL_TIMEOUT}s, continuing with timeout notice")
            # è¿”å›ä¸€ä¸ªè®©Agentæ„è¯†åˆ°è¶…æ—¶å¹¶ç»§ç»­çš„å“åº”ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                "thought": f"ä¸Šä¸€æ­¥LLMè°ƒç”¨è¶…æ—¶ï¼ˆ{settings.AGENT_LLM_CALL_TIMEOUT}ç§’ï¼‰ï¼Œéœ€è¦è°ƒæ•´ç­–ç•¥ç»§ç»­æ‰§è¡Œä»»åŠ¡ã€‚",
                "action": None  # æ²¡æœ‰actionï¼Œè®©ä¸‹ä¸€è½®è¿­ä»£é‡æ–°æ€è€ƒ
            }
        
        content = response.get("content", "")
        
        # è§£æ JSON å“åº”
        return self._parse_llm_response(content)
    
    def _parse_llm_response(self, content: str) -> Dict[str, Any]:
        """è§£æ LLM å“åº”ï¼Œæå– JSON"""
        json_str = ""
        
        # 1. å°è¯•æå– Markdown ä»£ç å—ä¸­çš„ JSON
        # ä¼˜åŒ–æ­£åˆ™ï¼šæ”¯æŒ json, jsonc æˆ–æ— è¯­è¨€æ ‡è®°ï¼Œæ”¯æŒå¤šè¡ŒåŒ¹é…
        json_match = re.search(r"```(?:json|jsonc)?\s*(\{.*?\})\s*```", content, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # 2. å°è¯•å¯»æ‰¾æœ€å¤–å±‚çš„ {}
            # ä½¿ç”¨æ ˆå¹³è¡¡æ¥æ‰¾åˆ°åŒ¹é…çš„æ‹¬å·ï¼Œæˆ–è€…ç®€å•åœ°æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
            start = content.find("{")
            end = content.rfind("}")
            if start != -1 and end != -1:
                json_str = content[start:end+1]
            else:
                json_str = content.strip()
        
        # æ¸…ç†å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯
        # 1. ç§»é™¤è¡Œå°¾é€—å· (ç®€å•å¤„ç†)
        # json_str = re.sub(r",\s*}", "}", json_str) # è¿™å¤ªå±é™©
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass

        # 3. å¦‚æœæ ‡å‡†è§£æå¤±è´¥ï¼Œå°è¯•ç”¨ dirtyjson æˆ– eval (ä¸å®‰å…¨ï¼Œè·³è¿‡) 
        # å°è¯•ä¿®å¤ï¼šæœ‰æ—¶ LLM ä¼šåœ¨ key ä¸­åŒ…å«æ¢è¡Œï¼Œæˆ–è€… value ä¸­åŒ…å«æœªè½¬ä¹‰çš„å¼•å·
        
        logger.warning(f"JSONDecodeError, attempting regex extraction. Content prefix: {content[:100]}")
        
        fallback_result = {}
        
        # æå– thought
        thought_match = re.search(r'"thought"\s*:\s*"(.*?)"', json_str, re.DOTALL)
        if thought_match:
            fallback_result["thought"] = thought_match.group(1)
            
        # æå– final_answer (æœ€é‡è¦)
        final_answer_match = re.search(r'"final_answer"\s*:\s*"(.*)"\s*}?\s*$', json_str, re.DOTALL)
        if final_answer_match:
            fallback_result["final_answer"] = final_answer_match.group(1)
            
        # å°è¯•æå– action
        if '"action"' in json_str:
             # æ­£åˆ™æå– action æ¯”è¾ƒéš¾ï¼Œè¿™é‡Œç®€å•å°è¯•
             pass
        
        if fallback_result:
            if "thought" not in fallback_result:
                fallback_result["thought"] = "ï¼ˆè§£ææ€è€ƒè¿‡ç¨‹æ—¶å‡ºé”™ï¼‰"
            return fallback_result

        # 4. å½»åº•å¤±è´¥ï¼Œåªèƒ½è¿”å›åŸå§‹æ–‡æœ¬
        logger.warning(f"Failed to parse LLM response as JSON: {content[:200]}")
        return {
            "thought": "JSON è§£æå¤±è´¥ï¼Œæ˜¾ç¤ºåŸå§‹å“åº”",
            "final_answer": content  # å°†åŸå§‹å“åº”ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        }
    
    def _is_duplicate_action(self, action: Dict[str, Any], history: List[Dict[str, Any]]) -> bool:
        """æ£€æŸ¥ action æ˜¯å¦åœ¨å†å²ä¸­é‡å¤å‡ºç°"""
        tool = action.get("tool")
        args = action.get("arguments", {})
        
        # ç®€å•åºåˆ—åŒ–å‚æ•°è¿›è¡Œæ¯”è¾ƒï¼Œé˜²æ­¢å­—å…¸é¡ºåºé—®é¢˜
        try:
            args_str = json.dumps(args, sort_keys=True)
        except:
            args_str = str(args)

        for step in history:
            prev_actions = []
            if "actions" in step:
                prev_actions.extend(step["actions"])
            elif "action" in step:
                prev_actions.append(step["action"])
            
            for prev_action in prev_actions:
                if prev_action.get("tool") == tool:
                    prev_args = prev_action.get("arguments", {})
                    try:
                        prev_args_str = json.dumps(prev_args, sort_keys=True)
                    except:
                        prev_args_str = str(prev_args)
                        
                    if prev_args_str == args_str:
                        return True
        return False

    async def _handle_error(
        self,
        task: str,
        failed_action: Dict[str, Any],
        error_message: str
    ) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†é”™è¯¯ï¼Œå°è¯•æ¢å¤
        
        Returns:
            æ¢å¤ç­–ç•¥æˆ– None
        """
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ Agentï¼Œéœ€è¦å¤„ç†å·¥å…·è°ƒç”¨é”™è¯¯ã€‚"},
            {"role": "user", "content": ERROR_RECOVERY_PROMPT.format(
                task=task,
                failed_action=json.dumps(failed_action, ensure_ascii=False),
                error_message=error_message
            )}
        ]
        
        try:
            response = await self.llm_service.chat_completion(
                messages=messages,
                temperature=0.3,  # ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„æ¢å¤ç­–ç•¥
                max_tokens=1024
            )
            
            content = response.get("content", "")
            return self._parse_llm_response(content)
            
        except Exception as e:
            logger.error(f"Error recovery failed: {e}")
            return None
    
    async def _generate_final_answer(
        self,
        task: str,
        observations: List[Dict[str, Any]]
    ) -> AsyncGenerator[str, None]:
        """
        åŸºäºæ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆæµå¼ï¼‰
        
        Yields:
             ç”Ÿæˆçš„å†…å®¹ç‰‡æ®µ
        """
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦åŸºäºæ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"},
            {"role": "user", "content": FINAL_ANSWER_TEMPLATE.format(
                task=task,
                observations=format_observations(observations)
            )}
        ]
        
        has_content = False
        async for chunk in self.llm_service.stream_chat_completion(
            messages=messages,
            temperature=0.5,
            max_tokens=settings.AGENT_DEFAULT_MAX_TOKENS
        ):
            content = chunk.get("content", "")
            if content:
                has_content = True
                yield content
        
        if not has_content:
            yield "æ— æ³•ç”Ÿæˆç­”æ¡ˆ"
    
    # =========================================================================
    # Plan-ReAct æ··åˆæ¨¡å‹æ–¹æ³•
    # =========================================================================
    
    async def _generate_plan(
        self,
        task: str,
        available_tools: List[Dict[str, Any]]
    ) -> Optional[ExecutionPlan]:
        """
        ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        
        Args:
            task: ç”¨æˆ·ä»»åŠ¡
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            
        Returns:
            ExecutionPlan æˆ– Noneï¼ˆå¦‚æœä¸éœ€è¦è®¡åˆ’ï¼‰
        """
        tools_text = format_tool_list(available_tools)
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è§„åˆ’åŠ©æ‰‹ï¼Œå¸®åŠ©åˆ†æä»»åŠ¡å¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚"},
            {"role": "user", "content": PLAN_GENERATION_PROMPT.format(
                task=task,
                tools=tools_text
            )}
        ]
        
        try:
            response = await asyncio.wait_for(
                self.llm_service.chat_completion(
                    messages=messages,
                    temperature=0.3,  # ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è®¡åˆ’
                    max_tokens=1024
                ),
                timeout=settings.AGENT_LLM_CALL_TIMEOUT
            )
            
            content = response.get("content", "")
            plan_data = self._parse_llm_response(content)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·
            if not plan_data.get("needs_tools", True) and not plan_data.get("steps"):
                # ç®€å•ä»»åŠ¡ï¼Œè¿”å›ç©ºæ­¥éª¤çš„è®¡åˆ’ï¼Œä»¥ä¾¿è§¦å‘ç›´æ¥å›ç­”é€»è¾‘
                logger.info("Plan generation: Simple task, returning empty plan for direct answer")
                return ExecutionPlan(
                    goal=plan_data.get("goal", task),
                    approach=plan_data.get("approach", "Direct answer"),
                    steps=[],
                    estimated_iterations=1,
                    is_replanned=False
                )
            
            # æ„å»º ExecutionPlan
            steps = []
            for step_data in plan_data.get("steps", []):
                steps.append(PlanStep(
                    id=step_data.get("id", f"step_{len(steps)+1}"),
                    description=step_data.get("description", ""),
                    tool_hint=step_data.get("tool_hint"),
                    depends_on=step_data.get("depends_on", []),
                    status=PlanStepStatus.PENDING
                ))
            
            plan = ExecutionPlan(
                goal=plan_data.get("goal", task),
                approach=plan_data.get("approach", ""),
                steps=steps,
                estimated_iterations=plan_data.get("estimated_iterations", 3)
            )
            
            logger.info(f"Generated plan with {len(steps)} steps: {plan.goal}")
            return plan
            
        except asyncio.TimeoutError:
            logger.warning("Plan generation timeout, proceeding without plan")
            return None
        except Exception as e:
            logger.error(f"Plan generation failed: {e}")
            return None
    
    async def _check_completion(
        self,
        task: str,
        plan: Optional[ExecutionPlan],
        observations: List[Dict[str, Any]],
        current_answer: str
    ) -> CompletionResult:
        """
        æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
        
        Args:
            task: åŸå§‹ä»»åŠ¡
            plan: æ‰§è¡Œè®¡åˆ’ï¼ˆå¯èƒ½ä¸º Noneï¼‰
            observations: æ”¶é›†çš„è§‚å¯Ÿç»“æœ
            current_answer: å½“å‰ç”Ÿæˆçš„å›ç­”
            
        Returns:
            CompletionResult
        """
        # æ ¼å¼åŒ–è®¡åˆ’æ‘˜è¦
        plan_summary = self._format_plan_summary(plan) if plan else "æ— é¢„è®¾è®¡åˆ’"
        
        # æ ¼å¼åŒ–è§‚å¯Ÿæ‘˜è¦
        obs_summary = format_observations(observations) if observations else "æ— æ”¶é›†ä¿¡æ¯"
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡å®Œæˆåº¦è¯„ä¼°åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": COMPLETION_CHECK_PROMPT.format(
                task=task,
                plan=plan_summary,
                observations_summary=obs_summary,
                current_answer=current_answer[:2000]  # é™åˆ¶é•¿åº¦
            )}
        ]
        
        try:
            response = await asyncio.wait_for(
                self.llm_service.chat_completion(
                    messages=messages,
                    temperature=0.2,  # æä½æ¸©åº¦ä»¥è·å¾—ç¡®å®šçš„åˆ¤æ–­
                    max_tokens=512
                ),
                timeout=30  # å®Œæˆåº¦æ£€æŸ¥åº”è¯¥å¿«é€Ÿ
            )
            
            content = response.get("content", "")
            result_data = self._parse_llm_response(content)
            
            return CompletionResult(
                is_complete=result_data.get("is_complete", True),
                confidence=result_data.get("confidence", 0.5),
                reasoning=result_data.get("reasoning", ""),
                missing_items=result_data.get("missing_items", []),
                suggested_next_steps=result_data.get("suggested_next_steps", [])
            )
            
        except Exception as e:
            logger.error(f"Completion check failed: {e}")
            # é»˜è®¤è®¤ä¸ºå®Œæˆ
            return CompletionResult(
                is_complete=True,
                confidence=0.3,
                reasoning=f"å®Œæˆåº¦æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
    
    def _format_plan_summary(self, plan: ExecutionPlan) -> str:
        """æ ¼å¼åŒ–è®¡åˆ’æ‘˜è¦ç”¨äºæç¤ºè¯"""
        lines = [
            f"**ç›®æ ‡ï¼š** {plan.goal}",
            f"**æ€è·¯ï¼š** {plan.approach}",
            "**æ­¥éª¤ï¼š**"
        ]
        
        for i, step in enumerate(plan.steps, 1):
            status_icon = {
                PlanStepStatus.PENDING: "â³",
                PlanStepStatus.IN_PROGRESS: "ğŸ”„",
                PlanStepStatus.DONE: "âœ…",
                PlanStepStatus.FAILED: "âŒ",
                PlanStepStatus.SKIPPED: "â­ï¸"
            }.get(step.status, "â³")
            
            tool_info = f" (ä½¿ç”¨ {step.tool_hint})" if step.tool_hint else ""
            lines.append(f"{i}. {status_icon} {step.description}{tool_info}")
            
            if step.result_summary:
                lines.append(f"   â†’ {step.result_summary}")
        
        return "\n".join(lines)


# å…¨å±€ ReAct å¼•æ“å®ä¾‹
react_engine = ReActEngine()
